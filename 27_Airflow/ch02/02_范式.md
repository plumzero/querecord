
[参考](https://airflow.apache.org/docs/apache-airflow/2.1.3/tutorial_taskflow_api.html)

本主题对使用 `Taskflow API` 范式编写 Pipeline 进行说明。`Taskflow API` 范式自 Airflow 2.0 开始引入。

这里所说的 Pipeline 为 ETL 模式，它由三个单独的任务组成，分别是: `Extract`、`Transform` 以及 `Load`。

### 'Taskflow API' ETL Pipeline

这里以一个使用了 Taskflow API 范式的简单的 ETL Pipeline 作为示例，看[这里](t/tutorial_taskflow_api_etl.py)。

我们可以根据这个文件创建一个 DAG 实例，这个脚本会被 Airflow 解释为 Pipeline 的配置文件。对于更为完整的 DAG 文件定义，请参考[导引](01_导引.md)。


### 实例化 DAG

创建后的 DAG 就像一个任务(task)容器，任务与任务之间相互独立。这里定义的 DAG 结构比较简单，没有各种重试或者复杂的调度操作。

在这里我们通过 `@dag` 修饰来进行 DAG 的创建，使用 python 函数名(这里是 `tutorial_taskflow_api_etl`)称作为 DAG 的标识符。
```py
    @dag(default_args=default_args, schedule_interval=None, start_date=days_ago(2), tags=['example'])
    def tutorial_taskflow_api_etl():
        """
        ### TaskFlow API Tutorial Documentation
        This is a simple ETL data pipeline example which demonstrates the use of
        the TaskFlow API using three simple tasks for Extract, Transform, and Load.
        Documentation that goes along with the Airflow TaskFlow API tutorial is
        located
        [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)
        """
```

### Tasks

在 Pipeline 中，任务通过 `@task` 修饰符并基于 Python 函数创建，函数名作为任务的唯一标识符。
```py
@task()
def extract():
    """
    #### Extract task
    A simple Extract task to get data ready for the rest of the data
    pipeline. In this case, getting data is simulated by reading from a
    hardcoded JSON string.
    """
    data_string = '{"1001": 301.27, "1002": 433.21, "1003": 502.22}'

    order_data_dict = json.loads(data_string)
    return order_data_dict
```
任务完成后会返回一个字典，它可能在后续任务中有用。

Transform 和 Load 任务以同样的方式创建。

### DAG 主流

我们已经创建了 Extract, Transform, Load 三个任务，现在将它们纳入到 DAG 的主流中去。
```py
    order_data = extract()
    order_summary = transform(order_data)
    load(order_summary["total_order_value"])
```
好的，完成了! 我们从 Extract 任务那里获取数据，将其发送到 Transform 任务进行处理，之后将数据发送给 Load 任务。任务与任务之间相互独立，每个任务都可以运行在不同节点上、不同的 worker 进程中。

最后，我们通过如下代码使这个 DAG 生效并可以被 Airflow 运行。
```py
    tutorial_etl_dag = tutorial_taskflow_api_etl()
```

###  多输出推断 multiple_outputs

任务可以通过 `dict python typing` 来推断多个输出:
```py
    @task
    def identify_dict(x: int, y: int) -> Dict[str, int]:
        return { "x": x, "y": y }
```
通过对函数返回类型使用`typing Dict`，`multiple_outputs` 参数会自动设置为 true。

### 为常规任务添加依赖进行修饰

上面所说的任务都是 python-based 类型的任务。事实上，在 python-based 任务运行之前很可能还会运行有其他(如通过 BashOperator 或 FileSensor 创建的)任务。
```py
    @task()
    def extract_from_file():
        """
        #### Extract from file task
        A simple Extract task to get data ready for the rest of the data
        pipeline, by reading the data from a file into a pandas dataframe
        """
        order_data_file = '/tmp/order_data.csv'
        order_data_df = pd.read_csv(order_data_file)


    file_task = FileSensor(task_id='check_file', filepath='/tmp/order_data.csv')
    order_data = extract_from_file()

    file_task >> order_data
```
在上面的代码块中，python-based 任务为 extract_from_file，在 DAG 主流中，还有一个 FileSensor 任务。
