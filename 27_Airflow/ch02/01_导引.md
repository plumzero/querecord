
[参考](https://airflow.apache.org/docs/apache-airflow/2.1.3/tutorial.html)

该主题主要简单带你了解在使用 Airflow 时写 `Pipeline` 会遇到的一些基本的概念、对象以及它们的用法。

Pipeline 在 Airflow 是一个很重要的概念，它其实就是用于定义一个 Airflow DAG 对象的一个 Python 脚本。这个脚本也可以称为 DAG 定义文件。

关于 DAG 和 Pipeline 的关系，可以将 DAG 看作是世界观，Pipleline 看作是方法论。

以这个[DAG定义文件](t/tutorial.py)作为示例。


### 关于 DAG 文件 

乍看这个脚本文件就像一个配置文件，只是它以代码的形式指定了 DAG 的结构。而在脚本中所定义的任务将会运行在一个与这个脚本文件不一样的上下文环境里。不同的任务适时地运行在不同节点的 worker 进程中，也就意味着这个脚本不能被用作跨任务通信。

可以这样理解，如果你想要定义两个任务，其中一个任务的部分输出将会作为另一个任务的输入。为了实现这种联系，你可以将这两个任务放在同一个 DAG 文件中。否则，就必须使用[XComs](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)挂机来实现跨脚本通信。

有的使用者会将 DAG 定义文件单纯地用作数据处理，但事实并非如此。这个脚本文件的目的是为了定义一个 DAG 对象，它需要以秒级时间创建，因为调度器 scheduler 周期性地将变化进行反映到这个对象上。而如果将其用作数据处理的话，很可能需要消耗可观的时间。


### 引入模块

Airflow Pipeline 就是一个 Python 脚本，可以用来定义一个 Airflow DAG 对象。

在定义一个 DAG 对象时，需要引入一些模块。如下示例:
```py
    from datetime import timedelta
    from textwrap import dedent

    # DAG 对象: 用于实例化一个 DAG
    from airflow import DAG

    # Operators: 需要这些来操作
    from airflow.operators.bash import BashOperator
    from airflow.utils.dates import days_ago
```


### Default Arguments

现在我们准备创建一个 DAG 文件以及几个任务，这时如果我们想要为每个任务的构造函数显式地传入一组参数(未必都能用到)，就可以定义一个包含默认参数的字典，用于创建任务:
```py
    # defaut_args 中的参数会传递到每个 operator 中，你可以在 operator 初始化时为每个任务重写这些参数
    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'email': ['airflow@example.com'],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
        # 'wait_for_downstream': False,
        # 'dag': dag,
        # 'sla': timedelta(hours=2),
        # 'execution_timeout': timedelta(seconds=300),
        # 'on_failure_callback': some_function,
        # 'on_success_callback': some_other_function,
        # 'on_retry_callback': another_function,
        # 'sla_miss_callback': yet_another_function,
        # 'trigger_rule': 'all_success'
    }
```
`BaseOperator` 会应用 `default_args` 的参数，其他的 Operator 会使用 BaseOperator 。

关于上面每个参数的意义，看[这里](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/index.html)。

这里对几个常用的参数进行说明:

| 参数名称 | 参数类型 | 说明 |
|:--------|:--------|:-----|
| task_id | str     | 作为任务的唯一标识 |
| owner   | str     | 任务的所有者，建议使用 unix 用户，一般是 airflow 用户 |
| description | str | DAG 描述，会显示在 web 页面上 |
| schedule_interval | datetime.timedelta/dateutil.relativedelta.relativedelta/str | 定义 DAG 多长时间执行一次，这个参数会被追加到最近一次执行产生的 execution_date 上以计算出下次调度的时刻 |
| email   | str 或 list[str] | 定义邮件报警的目的地址，可以是一个或多个，多个时以逗号或分号间隔 |
| email_on_retry | bool | 任务重试时，邮件报警是否也要重发 |
| email_on_failure | bool | 任务失败时，是否要发送邮件报警 |
| retries | int | 指定任务的重试测试 |
| retry_delay | datetime.timedelta | 指定两次重试之间的间隔 |
| start_date | datetime.datetime | 指定任务的开始时间 |
| end_date | datetime.datetime | 如果指定了这个参数，在超过这个时间后，调度器将不会对任务进行调度 |
| depends_on_past | bool | 设置为真时，任务实例循序执行，只有当前面的实例成功或被跳过时，才会执行下一个 |
| queue | str | 指定运行作业的队列。未必所有的执行器都实现了队列管理，比如 CeleryExecutor 就不支持 |
| pool | str | 指定任务运行的 slot pool 。用于限制某些任务的并发执行 |
| priority_weight | int | 指定相比于其他任务的优先权重。执行器会优先执行高权重任务 |
| dag | airflow.models.DAG | 对与任务绑定的 dag 对象的引用 |
| execution_timeout | datetime.timedelta | 设定任务实例的最大执行时长，若超时将抛出异常并失败 |
| on_failure_callback | TaskStateChangeCallback | 当一个任务实例失败时，将调用此函数 |
| on_success_callback | TaskStateChangeCallback | 当一个任务实例成功时，将调用此函数 |
| on_retry_callback  | TaskStateChangeCallback | 当一个任务进行触发重试时，将调用此函数 |
| trigger_rule | str | 一般为 all_success |
| tags | list[str] | 用于帮助在 web 页面上过滤 DAG |
| dag_id | str 或 list[str] | 用于查找一个 DAG 对象 |

使用者可以根据自己的意图来定义不同的参数，比如区分生产环境和开发环境。


### 实例化 DAG

我们需要创建 DAG 对象来将任务(task)放到里面运行，每个 DAG 对象可以通过一个名为 `dag_id` 的字符串进行唯一标识。

如下示例中，第一个参数 `tutorial` 即 dag_id 作为唯一标识，同时传入缺省参数集来定义一个执行频率为 1 天的 DAG 对象:

```py
    with DAG(
        'tutorial',
        default_args = default_args,            // 使用系统缺省参数集
        # 以下是重写的参数
        description = 'A simple tutorial DAG',
        schedule_interval = timedelta(days=1),
        start_date=days_ago(2),
        tags = ['example'],
    ) as dag:
```


### 任务

当实例化 operator 对象时，任务(task)就会被创建。也可以这样认为 operator 就是任务的构造函数。

我们将上面的参数传入到 operator 中，就可以创建一个任务，而 `task_id` 会作为这个任务的唯一标识:
```py
    t1 = BashOperator(
        task_id = 'print_date',
        bash_command = 'date',
    )

    t2 = BashOperator(
        task_id = 'sleep',
        depends_on_past = False,
        bash_command = 'sleep 5',
        retries = 3,
    )
```
这里在创建任务 t2 时，同时传入了一个针对于 BashOperator 的特定参数 bash_command，以及一个通用参数 retries，后者会对 default_args 中的同名参数进行重写。

在构造任务时，operator 有自己的缺省参数，也可以使用 default_args 中的参数，也可以显示传入参数。这三类参数的优先使用顺序如下:
1. 显示入参
2. 定义在 default_args 中的参数
3. operator 的缺省参数，如果存在的话

任务必须包括或者继承 `task_id` 和 `owner` 参数，否则 Airflow 会抛出异常。


### 使用 Jinja 模板

暂略。

[Jinja的使用参数](https://jinja.palletsprojects.com/en/latest/)


### 设置依赖

假定现在有 3 个任务，分别是 t1 t2 t3，开始时互不依赖。这里提供了几种方式来定义它们之间的依赖。

t2 依赖 t1，用箭头表示就是 t1 指向 t2。如下两种方式都可以表述这种关系:
```py
    t1.set_downstream(t2)
    t2.set_upstream(t1)
```

上面的不够直观，可以用下面的方式替代:
```py
    t1 >> t2
    t2 << t1
```

其他的依赖方式示例:
```py
    t1 >> t2 >> t3
    t1 >> [t2, t3]
    [t2, t3] >> t1
```

设置依赖时，注意不要形成闭环哟！


### 查看 DAG 文件

DAG 文件可以到配置文件配置项 `core.dags_folder` 指定路径下查看，也可以通过如下命令行查看。

列出活跃的 DAG:
```sh
    airflow dags list
```

列出 `tutorial` DAG 中的任务:
```sh
    airflow tasks list tutorial
```

树状形式列出 `tutorial` DAG 中的任务:
```sh
    airflow tasks list tutorial --tree
```

### 任务补录 backfill

到现在为上，一切看起来进展的都很顺利。现在来谈一谈任务补录吧(backfill)。

`backfill` 会遵照你的 Airflow 部署依赖，将日志注入文件的同时也会与数据库交互将状态进行记录保存。如果你启动了 webserver 进程服务，那么就可以在页面上追踪这些状态。

如果在构造任务使用参数 `depends_on_past=True` 时，是否执行任务实例要依赖于上一个任务实例是否执行成功(也就是说，取决于上一个任务实例的`execution_date`)。如果任务实例参数为 `execution_date==start_date`，那么运行会忽略这种依赖，因为对这个实例来说，已经没有过往的任务实例可言。

在使用 `depends_on_past=True` 参数时，你可能还需要考虑 `wait_for_downstream=True` 参数。`depends_on_past=True` 让(两个任务实例中的)一个实例等待上一个实例成功，`wait_for_downstream=True` 也会让任务实例等待前一个任务实例下游的所有任务实例成功。

上下文环境中的日期范围以 `start_date` 和视需要选择的 `end_date` 进行定义。设定之后，DAG 对象中位于该日期范围内的任务实例都会被调度执行。

示例:
```sh
    airflow dags backfill tutorial --start-date 2021-09-11 --end-date 2021-09-15
```

