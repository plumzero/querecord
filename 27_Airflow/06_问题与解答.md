
### 用于测试的 DAG 未消除

描述:
在配置中将 `load_examples` 设置为 `False` 后，重启 webserver，在页面中仍存在用于测试的 DAG 。

解决:
在 webserver 启动之前重置一下数据库:
```sh
    airflow$ airflow db reset
```


### 新添加的 DAG 未在 web 页面上显示

描述:
将 DAG 文件放置到 `dags_folder` 指定目录下后，刷新 web 页面，并没有显示出来。

解决:
不能总是 reset 数据库啊，把 `scheduler` 进程启动起来就好了。
```sh
    airflow$ airflow scheduler
```


### 启动 `scheduler` 进程时报错

报错如下:
```sh
    psycopg2.errors.InsufficientPrivilege: permission denied for table celery_taskmeta
```

以数据库超级用户 postgres 进入 `celery` 库，为其赋予权限:
```sh
    celery=# GRANT ALL PRIVILEGES ON TABLE celery_taskmeta TO airflower;
    GRANT
```
这个错误一般是不会发生的，除非你用的不是 airflower 用户初始化的数据库。


### 启动 `worker` 进程时报错

情况一:

报错如下:
```sh
    psycopg2.errors.InsufficientPrivilege: permission denied for sequence task_id_sequence
```

以数据库超级用户 postgres 进入 `celery` 库，为其赋予权限:
```sh
    celery=# GRANT ALL PRIVILEGES ON SEQUENCE task_id_sequence TO airflower;
    GRANT
```
这个错误一般是不会发生的，除非你用的不是 airflower 用户初始化的数据库。

情况二:

```txt
    Traceback (most recent call last):
    File "/home/airflow/anaconda3/bin/airflow", line 8, in <module>
        sys.exit(main())
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/__main__.py", line 40, in main
        args.func(args)
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 47, in command
        func = import_string(import_path)
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
        module = import_module(module_path)
    File "/home/airflow/anaconda3/lib/python3.8/importlib/__init__.py", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
    File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
    File "<frozen importlib._bootstrap>", line 991, in _find_and_load
    File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
    File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
    File "<frozen importlib._bootstrap_external>", line 783, in exec_module
    File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/cli/commands/celery_command.py", line 29, in <module>
        from flower.command import FlowerCommand
    ImportError: cannot import name 'FlowerCommand' from 'flower.command' (/home/airflow/anaconda3/lib/python3.8/site-packages/flower/command.py)
```

可能是 `flower` 命令没有安装成功的缘故，重新安装一下:
```sh
    airflow$ pip install flower redis redis celery==4.4.7
```


### 替换或修改 DAG 文件

删除、替换或修改 DAG 文件时，最好不要立即删除 `dags_folder` 目录下的文件，而应该采取这样的步骤:
1. 在 web 页面上点击垃圾桶图标(Delete DAG)进行删除
2. 之后再对 `dags_folder` 下的相应文件进行删除、替换或修改

`scheduler` 进程会定时轮询 `dags_folder` 目录，所以一段时间后，刷新页面就可以看到又显示到页面上了。

`scheduler` 的轮询时间可以通过 airflow.cfg 中的 `scheduler.dag_dir_list_interval` 配置项进行修改，默认为 300 秒。

### 执行 DAG 文件中任务一直处理 running 状态

是不是没有启动 `worker` 进程~


### 数据库初始化时报错
 
报错内容如下:
```sh
    ERROR [flask_caching.backends.filesystemcache] set key '__wz_cache_count' -> [Errno 1] Operation not permitted: '/tmp/tmp2zjpe88l.__wz_cache' -> '/tmp/2029240f6d1128be89ddc32729463129'
```

产生场景: 在同一台机器上配置两套 Airflow 环境，在启动第二套时报错。

解决: 启动第一套后，删除 `/tmp/2029240f6d1128be89ddc32729463129` 文件。


### 修改时区问题

问题描述见这里: [Airflow修改时区问题](https://www.codenong.com/js072cbe35bc89/)

参考解决办法: 
* [Airflow1.10.4介绍与安装](https://www.cnblogs.com/woshimrf/p/airflow-install-with-docker.html)
* [Airflow 2.1.4 本地时区问题解决](https://blog.csdn.net/qq_33682575/article/details/120842104)

在 Airflow 2.1.3 上修改过程如下。

1. 更改配置文件
```cfg
    [core]
    default_timezone = system
```

2. 更改源码文件

进入 airflow 安装位置，也就是 site-packages 中的airflow，这里是 `/home/airflow/anaconda3/lib/python3.8/site-packages/airflow`。

- 修改 `utils/timezone.py` 文件:

在 `utc = pendulum.tz.timezone('UTC')` (第27行)代码下添加，同时注释该行:
```py
    # utc = pendulum.tz.timezone('UTC')
    from airflow.configuration import conf
    try:
        tz = conf.get("core", "default_timezone")
        if tz == "system":
            utc = pendulum.local_timezone()
        else:
            utc = pendulum.timezone(tz)
    except Exception:
        pass
```

修改 `utcnow()` 函数(第70行):
```py
    # result = dt.datetime.utcnow()
    result = dt.datetime.now()
```

- 修改 `utils/sqlalchemy.py` 文件:

在 `utc = pendulum.tz.timezone('UTC')` (第35行)代码下添加，同时注释该行:
```py
    # utc = pendulum.tz.timezone('UTC')
    try:
        tz = conf.get("core", "default_timezone")
        if tz == "system":
            utc = pendulum.local_timezone()
        else:
            utc = pendulum.timezone(tz)
    except Exception:
        pass
```

集群或者不同用户下可能需要一个一个的修改，比较麻烦。

### 界面显示调度器未运行

```sh
    The scheduler does not appear to be running. Last heartbeat was received in 7 hours.
    The DAGs list may not update, and new tasks will not be scheduled.
```
但是实际上 scheduler 已经在运行了。

其实这个问题是修改时区留下的后遗症，因为我将时区从 utc 时间调整到 utc-8 时间后，时间点前移 8 个小时，但是数据库里的心跳时间戳还停留在原地，所以因为假超时 Airflow 判定 scheduer 未在运行。

料想在几个小时之后应该就会恢复正常(即上面的提示消失)。


### 后台启动 airflow 进程失败

使用其他机器上的数据库，修改 airflow.cfg 相关配置项，之后尝试前台启动执行 `airflow webserver` 命令，没什么问题。但是添加参数 `-D` 再执行，后台并没有相关进程在运行。

解决办法: 进入 `$AIRFLOW_HOME` 目录下，将一些垃圾文件删除。垃圾文件具体是指除了 airflow.cfg、logs、dags、webserver_config.py 之外的一些 airflow 产生的一些进程、输出、日志等文件。


### Airflow 配置时区选项务必要填入明确时区

在 airflow.cfg 中有 `core.default_timezone` 项，很多人会赋值为 `system` 表示跟随系统。但是有时候会出现一些问题，那就是有些系统可能并不是我们一眼就能明白的标准时区格式。如下:
```sh
    $ timedatectl 
        Local time: Wed 2021-10-20 19:19:32 CST
    Universal time: Wed 2021-10-20 11:19:32 UTC
            RTC time: Wed 2021-10-20 11:19:32
        Time zone: Etc/UTC (CST, +0800)
    Network time on: yes
    NTP synchronized: yes
    RTC in local TZ: no
```
上面的 Time zone，虽然也表示 utc-8，但在 DAG 中设置调度时，可能会无法识别。

所以 `core.default_timezone` 更建议设置明确的时区项，如 `Asia/Shanghai`。
