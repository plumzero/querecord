
### 用于测试的 DAG 未消除

描述:
在配置中将 `load_examples` 设置为 `False` 后，重启 webserver，在页面中仍存在用于测试的 DAG 。

解决:
在 webserver 启动之前重置一下数据库:
```sh
    airflow$ airflow db reset
```


### 新添加的 DAG 未在 web 页面上显示

描述:
将 DAG 文件放置到 `dags_folder` 指定目录下后，刷新 web 页面，并没有显示出来。

解决:
不能总是 reset 数据库啊，把 `scheduler` 进程启动起来就好了。
```sh
    airflow$ airflow scheduler
```


### 启动 `scheduler` 进程时报错

报错如下:
```sh
    psycopg2.errors.InsufficientPrivilege: permission denied for table celery_taskmeta
```

以数据库超级用户 postgres 进入 `celery` 库，为其赋予权限:
```sh
    celery=# GRANT ALL PRIVILEGES ON TABLE celery_taskmeta TO airflower;
    GRANT
```
这个错误一般是不会发生的，除非你用的不是 airflower 用户初始化的数据库。


### 启动 `worker` 进程时报错

情况一:

报错如下:
```sh
    psycopg2.errors.InsufficientPrivilege: permission denied for sequence task_id_sequence
```

以数据库超级用户 postgres 进入 `celery` 库，为其赋予权限:
```sh
    celery=# GRANT ALL PRIVILEGES ON SEQUENCE task_id_sequence TO airflower;
    GRANT
```
这个错误一般是不会发生的，除非你用的不是 airflower 用户初始化的数据库。

情况二:

```txt
    Traceback (most recent call last):
    File "/home/airflow/anaconda3/bin/airflow", line 8, in <module>
        sys.exit(main())
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/__main__.py", line 40, in main
        args.func(args)
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 47, in command
        func = import_string(import_path)
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
        module = import_module(module_path)
    File "/home/airflow/anaconda3/lib/python3.8/importlib/__init__.py", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
    File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
    File "<frozen importlib._bootstrap>", line 991, in _find_and_load
    File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
    File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
    File "<frozen importlib._bootstrap_external>", line 783, in exec_module
    File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
    File "/home/airflow/anaconda3/lib/python3.8/site-packages/airflow/cli/commands/celery_command.py", line 29, in <module>
        from flower.command import FlowerCommand
    ImportError: cannot import name 'FlowerCommand' from 'flower.command' (/home/airflow/anaconda3/lib/python3.8/site-packages/flower/command.py)
```

可能是 `flower` 命令没有安装成功的缘故，重新安装一下:
```sh
    airflow$ pip install flower redis redis celery==4.4.7
```


### 替换或修改 DAG 文件

删除、替换或修改 DAG 文件时，最好不要立即删除 `dags_folder` 目录下的文件，而应该采取这样的步骤:
1. 在 web 页面上点击垃圾桶图标(Delete DAG)进行删除
2. 之后再对 `dags_folder` 下的相应文件进行删除、替换或修改

`scheduler` 进程会定时轮询 `dags_folder` 目录，所以一段时间后，刷新页面就可以看到又显示到页面上了。

`scheduler` 的轮询时间可以通过 airflow.cfg 中的 `scheduler.dag_dir_list_interval` 配置项进行修改，默认为 300 秒。

### 执行 DAG 文件中任务一直处理 running 状态

是不是没有启动 `worker` 进程~


### 数据库初始化时报错
 
报错内容如下:
```sh
    ERROR [flask_caching.backends.filesystemcache] set key '__wz_cache_count' -> [Errno 1] Operation not permitted: '/tmp/tmp2zjpe88l.__wz_cache' -> '/tmp/2029240f6d1128be89ddc32729463129'
```

产生场景: 在同一台机器上配置两套 Airflow 环境，在启动第二套时报错。

解决: 启动第一套后，删除 `/tmp/2029240f6d1128be89ddc32729463129` 文件。


### 修改时区问题

问题描述见这里: [Airflow修改时区问题](https://www.codenong.com/js072cbe35bc89/)

参考解决办法: 
* [Airflow1.10.4介绍与安装](https://www.cnblogs.com/woshimrf/p/airflow-install-with-docker.html)
* [Airflow 2.1.4 本地时区问题解决](https://blog.csdn.net/qq_33682575/article/details/120842104)

在 Airflow 2.1.3 上修改过程如下。

1. 更改配置文件
```cfg
    [core]
    default_timezone = system
```

2. 更改源码文件

进入 airflow 安装位置，也就是 site-packages 中的airflow，这里是 `/home/airflow/anaconda3/lib/python3.8/site-packages/airflow`。

- 修改 `utils/timezone.py` 文件:

在 `utc = pendulum.tz.timezone('UTC')` (第27行)代码下添加，同时注释该行:
```py
    # utc = pendulum.tz.timezone('UTC')
    from airflow.configuration import conf
    try:
        tz = conf.get("core", "default_timezone")
        if tz == "system":
            utc = pendulum.local_timezone()
        else:
            utc = pendulum.timezone(tz)
    except Exception:
        pass
```

修改 `utcnow()` 函数(第70行):
```py
    # result = dt.datetime.utcnow()
    result = dt.datetime.now()
```

- 修改 `utils/sqlalchemy.py` 文件:

在 `utc = pendulum.tz.timezone('UTC')` (第35行)代码下添加，同时注释该行:
```py
    # utc = pendulum.tz.timezone('UTC')
    try:
        tz = conf.get("core", "default_timezone")
        if tz == "system":
            utc = pendulum.local_timezone()
        else:
            utc = pendulum.timezone(tz)
    except Exception:
        pass
```

集群或者不同用户下可能需要一个一个的修改，比较麻烦。

### 页面显示时间

airflow.cfg 中的 `webserver.default_ui_timezone` 用于设置页面显示时间，最好还是设置为具体时区吧。如下:
```cfg
    [webserver]
    default_ui_timezone = Asia/Shanghai
```

### 界面显示调度器未运行

```sh
    The scheduler does not appear to be running. Last heartbeat was received in 7 hours.
    The DAGs list may not update, and new tasks will not be scheduled.
```
但是实际上 scheduler 已经在运行了。

其实这个问题是修改时区留下的后遗症，因为我将时区从 utc 时间调整到 utc-8 时间后，时间点前移 8 个小时，但是数据库里的心跳时间戳还停留在原地，所以因为假超时 Airflow 判定 scheduer 未在运行。

料想在几个小时之后应该就会恢复正常(即上面的提示消失)。


### 后台启动 airflow 进程失败

使用其他机器上的数据库，修改 airflow.cfg 相关配置项，之后尝试前台启动执行 `airflow webserver` 命令，没什么问题。但是添加参数 `-D` 再执行，后台并没有相关进程在运行。

解决办法: 进入 `$AIRFLOW_HOME` 目录下，将一些垃圾文件删除。垃圾文件具体是指除了 airflow.cfg、logs、dags、webserver_config.py 之外的一些 airflow 产生的一些进程、输出、日志等文件。


### Airflow 配置时区选项务必要填入明确时区

在 airflow.cfg 中有 `core.default_timezone` 项，很多人会赋值为 `system` 表示跟随系统。但是有时候会出现一些问题，那就是有些系统可能并不是我们一眼就能明白的标准时区格式。如下:
```sh
    $ timedatectl 
        Local time: Wed 2021-10-20 19:19:32 CST
    Universal time: Wed 2021-10-20 11:19:32 UTC
            RTC time: Wed 2021-10-20 11:19:32
        Time zone: Etc/UTC (CST, +0800)
    Network time on: yes
    NTP synchronized: yes
    RTC in local TZ: no
```
上面的 Time zone，虽然也表示 utc-8，但在 DAG 中设置调度时，可能会无法识别。

所以 `core.default_timezone` 更建议设置明确的时区项，如 `Asia/Shanghai`。


### dag_id 找不到

报错如下:
```sh
    airflow.exceptions.AirflowException: dag_id could not be found: queue_example. Either the dag did not exist or it failed to parse.
    [2021-10-23 10:38:09,129: ERROR/ForkPoolWorker-2] Task airflow.executors.celery_executor.execute_command[edd7d8df-91a9-46f5-82a1-ffa3095a90f5] raised unexpected: AirflowException('Celery command failed on host: myhost-110')
    Traceback (most recent call last):
```

两个节点下的 DAG 文件不一致，不一致的内容如下。
一个节点为:
```sh
        t2cavim
```
另一个节点为:
```sh
        t2
```

不止要求名称相同，严格来说，使用者甚至应该保证每个节点下的对应 DAG 文件的 md5sum 也是一致的。

### 获取不到日志

报错如下:
```sh
    *** Log file does not exist: /home/airflow/airflow/logs/queue_example/task_at_231/2021-10-21T03:15:00+08:00/1.log
    *** Fetching from: http://host-231:8793/log/queue_example/task_at_231/2021-10-21T03:15:00+08:00/1.log
    *** Failed to fetch log file from worker. [Errno -2] Name or service not known
```
啊，是没有完成对主机名称 host-231 的映射啊。在 `/etc/hosts` 配置一下就行了。

之后，又报如下的错误
```
    *** Log file does not exist: /home/airflow/airflow/logs/queue_example/task_at_231/2021-10-21T03:15:00+08:00/1.log
    *** Fetching from: http://host-231:8793/log/queue_example/task_at_231/2021-10-21T03:15:00+08:00/1.log
    *** !!!! Please make sure that all your Airflow components (e.g. schedulers, webservers and workers) have the same 'secret_key' configured in 'webserver' section !!!!!
    ****** See more at https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#secret-key
    ****** Failed to fetch log file from worker. 403 Client Error: FORBIDDEN for url: http://host-231:8793/log/queue_example/task_at_231/2021-10-21T03:15:00+08:00/1.log
    For more information check: https://httpstatuses.com/403
```
403 说明 webserver 节点机器不具有对这个 worker 节点资源的访问权限。根据提示，尝试将每个节点下的配置项 `webserver.secret_key` 设置为相同的数值。重试，还是报相同的错误。

暂时不太清楚怎么回事，暂时搁置。

参考: [Chart: Create a random secret for Webserver's flask secret key](https://github.com/apache/airflow/pull/17142)

好的，最后还是 OK 了。原来 worker 进程在启动的时候，会创建一个 serve-logs 进程。这个进程与日志有关，之前将 worker 杀掉时，没有将这个 serve-logs 进程杀掉，然后该进程交由 init 进程托管，再次重启 worker 进程时，检测到 serve-logs 进程还在，所以就不再创建了。但是当前存在的 serve-logs 进程已经与新启动的 worker 进程没有关联了。

解决办法是在杀掉 worker 进程的同时，也将 serve-logs 进程杀掉。
