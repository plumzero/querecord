
[参考](https://airflow.apache.org/docs/apache-airflow/2.1.3/concepts/tasks.html)

任务(Task)是 Airflow 的基本执行单元，它在 DAG 中进行组织，通过上下游依赖来设置任务与任务之间应该运行的顺序。

任务的基本类型有 3 种，具体请查看[架构概述](01_架构概述.md)关于[Workloads](01_架构概述.md#Workloads)方面的内容。

### 关系 Relationships

具体请查看[架构概述](01_架构概述.md)关于[流的控制(Control Flow)](01_架构概述.md#流的控制)方面的内容。

### 任务实例 Task Instances

DAG 在每次运行时会实例化为一个 DAG Run，同样地，DAG 中的 Task 在每次运行时也会实例化为对应的 `Task Instances`。

Task 的一个实例，就是给定 DAG 中对应任务的一次特定运行(运行后会产生一个 `execution_date`)，也可以认为是具有(表示其所处生命周期的)状态的任务。

对于一个 Task 实例来说，可能的状态如下:
* `none`: 任务还未被入队执行(其依赖性未得到满足)
* `scheduled`: 调度器认为任务的依赖性已满足，应该运行了
* `queued`: 任务被分配到一个执行器(Executor)上，等待一个 worker 进程的运行
* `running`: 任务正在某个 worker 上运行(或者在本地/同步执行器上)
* `success`: 任务运行成功
* `failed`: 任务在运行期间因错误而失败
* `skipped`: 可能因为分支、LatestOnly或其他原因，导致任务被跳过
* `upstream_failed`: 因为上游任务运行失败而导致处于这种状态，实际上可能并不是这个任务的错
* `up_for_retry`: 任务虽然失败，但重试次数还没用完，接下来会继续安排调度
* `up_for_reschedule`: 任务类型为 `Sensor` 时，会有一个 `reschedule` 模式
* `sensing`: 任务是一个 `Smart Sensor`
* `removed`: 自运行开始以来，这个任务就从 DAG 消失了

![](img/task_lifecycle_diagram.png)

理想情况下，一个任务应该沿着 `none`、`scheduled`、`queued`、`running` 到 `success` 这条线运行。

### 超时 Timeouts

如果你想为任务设定一个最大运行时长，将 `execution_timeout` 属性赋值为一个 `datetime.timedelta` 类型值即可。如果任务实际运行超出了这个设定时长，Airflow 会踢开这个任务，并以超时异常将此任务设置为失败。

如果你仅仅想要在运行超时时只给出一个提示，让任务继续运行直到结束而不是强制进行失败终止，可以使用 `SLAs`。

### SLAs

SLA(Service Level Agreement) 是一个任务最长的预期时长成本。如果超出此时长，在 UI 界面 `Browse -> SLA Misses` 上可以看到一些提示，还可以通过电子邮件发送错失 SLA 的所有任务。

超过 SLA 的任务不会被取消，它们会被允许继续运行直到完成。

为 Task/Operator 的 `sla` 参数传递一个 `datetime.timedelta` 对象，可以达成为任务设置 SLA 的目的。你也可以提供一个 `sla_miss_callback` 回调，它会在错失 SLA 时被执行，回调内可以运行自己的逻辑。

如果你想完全禁用 SLA 检测，在 Airflow 配置中在 `[core]` 中设置 `check_slas = False`。

关于邮件配置，请查看[这里](https://airflow.apache.org/docs/apache-airflow/2.1.3/howto/email-config.html)。

### 特殊的异常

Airflow 提供了两个特殊异常用于在自定义 Task/Operator 代码中控制任务的状态:
* `AirflowSkipException`: 将当前任务状态设置为 skipped
* `AirflowFailException`: 忽略任何重试操作，将当前任务状态设置为 failed

如果你的代码中有额外的一些东西，那么通过这些异常设置任务状态就很有用。比如，当数据无效时应该跳过任务的执行，或者接口不可用时应该视作失败等等。

## 僵尸/不死任务

[参考](https://airflow.apache.org/docs/apache-airflow/2.1.3/concepts/tasks.html#zombie-undead-tasks)

不用担心，它们会被 Airflow 定时清理。不过它们的存在可能也让设定重试操作变得有意义起来。

### 执行器配置

[参考](https://airflow.apache.org/docs/apache-airflow/2.1.3/concepts/tasks.html#executor-configuration)
